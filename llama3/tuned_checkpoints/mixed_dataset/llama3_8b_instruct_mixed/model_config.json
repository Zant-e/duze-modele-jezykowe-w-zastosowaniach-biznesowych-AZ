{"model_name": "model", "model_type": "instruct", "quant_type": "nf4", "lora_ckpt_path": "", "output_dir": "tuned_checkpoints/llama3_8b_instruct_mixed", "lora_target": ["all_linear", "output"], "lora_r": 16, "lora_alpha": 128, "lora_dropout": 0.1, "batch_size_training": 1, "val_batch_size": 1, "context_length": 8192, "gradient_accumulation_steps": 64, "num_epochs": 1, "max_train_step": 99999999, "max_eval_step": 9999999, "lr": 0.0001, "weight_decay": 0, "gamma": 0.1, "batching_strategy": "padding", "gradient_clipping": false, "gradient_clipping_threshold": 1.0, "num_workers_dataloader": 1, "run_validation": true, "seed": 1, "dataset": "mixed", "save_model": true, "save_metrics": false, "use_wandb": true, "use_moe": false, "num_experts": 8, "num_experts_per_tok": 3, "penalty_alpha": 0.01}