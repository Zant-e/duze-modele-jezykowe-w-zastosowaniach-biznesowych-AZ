from llama import Llama
from lm_eval.api.model import LM
from typing import List, Tuple, Dict
import gc
import torch
from torch.functional import F


class HarnessModel(LM):
    def __init__(self, generator: Llama):
        super().__init__()
        self.generator = generator
        self.model = self.generator.model
        self.tokenizer = self.generator.tokenizer

    def generate_until(self, requests) -> List[str]:
        """Generate greedily until a stopping sequence

        :param requests: list[Instance]
            A list of Instance objects with property `args` which returns a tuple (context, until).
            context: str
                Context string
            until: [str]
                The string sequences to generate until. These string sequences
                may each span across multiple tokens, or may be part of one token.
        :return: list[str]
            A list of strings continuationa
            continuation: str
                The generated continuation.
        """

        self.model.eval()
        results = []
        for instance in requests:
            context, until = instance.args
            context_tokens = self.tokenizer.encode(context, bos=True, eos=False)
            until_tokens = until["until"]
            prompt_tokens = context_tokens

            with torch.no_grad():
                generation_tokens, _ = self.generator.generate(
                    prompt_tokens=[prompt_tokens],
                    max_gen_len=self.model.params.max_seq_len,
                    temperature=0.6,
                    top_p=0.9,
                    logprobs=False,
                    echo=False,
                    additional_stop_tokens=until_tokens,
                )
            results.append(self.tokenizer.decode(generation_tokens[0]))

            del generation_tokens
            gc.collect()
            torch.cuda.empty_cache()

        return results

    def loglikelihood(self, requests) -> List[Tuple[float, bool]]:
        """Compute log-likelihood of generating a continuation from a context.

        :param requests: list[Instance]
            A list of Instance objects, with property `args` which returns a tuple (context, continuation).
            `context: str`
                Context string. Implementations of LM must be able to handle an
                empty context string.
            `continuation: str`
                The continuation over which log likelihood will be calculated. If
                there is a word boundary, the space should be in the continuation.
                For example, context="hello" continuation=" world" is correct.

        :return: list[tuple[float, bool]]
            A list of pairs (logprob, isgreedy)
            `logprob: float`
                The log probability of `continuation`.
            `isgreedy`:
                Whether `continuation` would be generated by greedy sampling from `context`.
        """
        self.model.eval()
        results = []

        for request in requests:
            context, continuation = request.args

            # Encode the context and continuation
            context_tokens = self.tokenizer.encode(context, bos=True, eos=False)

            # eos=True doesn't work properly for lambada perplexity testing -> figure this out later
            continuation_tokens = self.tokenizer.encode(
                continuation, bos=False, eos=True
            )

            # continuation_tokens = self.tokenizer.encode(
            #     continuation, bos=False, eos=False
            # )
            input_tokens = context_tokens + continuation_tokens

            # Ensure the total length does not exceed the maximum sequence length
            if len(input_tokens) > self.model.params.max_seq_len:
                raise ValueError(
                    f"Input sequence length {len(input_tokens)} exceeds maximum sequence length {self.model.params.max_seq_len}"
                )

            input_tensor = torch.tensor([input_tokens], dtype=torch.long, device="cuda")

            with torch.no_grad():
                logits = self.model(input_tensor, start_pos=0)

            # Calculate log-probabilities for the continuation part
            logits = logits[0, len(context_tokens) - 1 : -1]
            continuation_tensor = torch.tensor(
                continuation_tokens, dtype=torch.long, device="cuda"
            )
            log_probs = -F.cross_entropy(logits, continuation_tensor, reduction="none")

            log_likelihood = log_probs.sum().item()

            greedy_tokens = torch.argmax(logits, dim=-1)
            is_greedy = torch.all(greedy_tokens == continuation_tensor).item()

            results.append((log_likelihood, bool(is_greedy)))

            del (
                input_tensor,
                logits,
                log_probs,
                log_likelihood,
                continuation_tensor,
                greedy_tokens,
                is_greedy,
                continuation_tokens,
                context_tokens,
            )
            gc.collect()
            torch.cuda.empty_cache()

        return results

    def loglikelihood_rolling(self, requests) -> List[Tuple[float]]:
        """Compute full log-likelihood of a string, with no truncation, for perplexity computation
        - We will use the full max context length of the model.
        - For inputs that exceed the max context length, we divide the tokenized string into chunks of up to
        the max context length.
        - IMPORTANT: Each document's loglikelihood/perplexity is computed *separately*, unlike other implementations
        which may simply concatenate multiple documents together.
        - IMPORTANT: We maximize the amount of context for each prediction. Specifically, for inputs that we break into
        multiple chunks, the last input will still a full-sized context.
        Example:
            Input tokens: [ 0 1 2 3 4 5 6 7 8 9 ]
            Prefix: BOS/EOS
            Max context length: 4
            Resulting input/prediction pairs:

                INPUT:  BOS   0   1   2
                PRED:     0   1   2   3

                INPUT:    3   4   5   6
                PRED:     4   5   6   7

                INPUT:    5   6   7   8
                PRED:             8   9

        Observe that:
            1. Each token is predicted exactly once
            2. For the last pair, we provide the full context, but only score the last two tokens

        :param requests: list[Instance]
            A list of Instance objects with property `args` which returns a tuple (context,).
            string: str
                String for which we are computing overall loglikelihood
        :return: list[tuple[float]]
            A list of tuples (logprob,)
            logprob: float
                The log probability of `context` conditioned on the BOS/EOS token.
                Can also be overridden for custom cases by `prefix_token_id`.
        """
        self.model.eval()
        results = []
        for request in requests:
            context = request.args[0]

            context_tokens = self.tokenizer.encode(context, bos=True, eos=False)
            max_context_length = self.model.params.max_seq_len

            total_logprob = 0.0

            for i in range(0, len(context_tokens), max_context_length):
                input_tokens = context_tokens[i : i + max_context_length]
                input_tensor = torch.tensor(
                    [input_tokens], dtype=torch.long, device="cuda"
                )

                # Forward pass to get logits
                with torch.no_grad():
                    logits = self.model(input_tensor, start_pos=0)

                # Calculate log-probabilities for the input tokens
                logits = logits[0, :-1]
                target_tokens = input_tensor[0, 1:]

                log_probs = -F.cross_entropy(logits, target_tokens, reduction="none")
                total_logprob += log_probs.sum().item()

            results.append((total_logprob,))

            del (
                input_tokens,
                input_tensor,
                logits,
                log_probs,
                total_logprob,
                target_tokens,
                context_tokens,
            )
            gc.collect()
            torch.cuda.empty_cache()
        return results
