from llama.generation import Llama
from lm_eval.api.model import LM
from typing import List, Tuple, Dict
import gc
import torch
from torch.functional import F


# func for creating a config and model
def instantiate_model_and_tokenizer(
    instruct_model: bool = True,
    max_seq_len: int = 128,  # <= 8192
    max_batch_size: int = 4,
    seed: int = 1,
    lora_target: list = [],
    lora_r: int = 0,
    lora_alpha: int = 1,
    lora_dropout: float = 0.0,
    quant_type: str = "",
    layers_to_upcast: list = [],
    output_requires_grad: bool = False,
):
    if quant_type == "":
        quant = None
    else:
        quant = quant_type

    params = {
        "dim": 4096,
        "n_layers": 32,
        "n_heads": 32,
        "n_kv_heads": 8,
        "vocab_size": 128256,
        "multiple_of": 1024,
        "ffn_dim_multiplier": 1.3,
        "norm_eps": 1e-05,
        "rope_theta": 500000.0,
        "lora_target": lora_target,
        "lora_r": lora_r,
        "lora_alpha": lora_alpha,
        "lora_dropout": lora_dropout,
        "quant_type": quant,
    }

    if instruct_model:
        ckpt_dir = "Meta-Llama-3-8B-Instruct"
        tokenizer_path = "Meta-Llama-3-8B-Instruct/tokenizer.model"
    else:
        ckpt_dir = "Meta-Llama-3-8B"
        tokenizer_path = "Meta-Llama-3-8B/tokenizer.model"

    generator = Llama.build(
        ckpt_dir=ckpt_dir,
        tokenizer_path=tokenizer_path,
        max_seq_len=max_seq_len,
        max_batch_size=max_batch_size,
        seed=seed,
        params=params,
    )

    generator.prep_for_training(
        layers_to_upcast=layers_to_upcast, output_requires_grad=output_requires_grad
    )

    return generator


class HarnessModel(LM):
    def __init__(self, generator: Llama):
        super().__init__()
        self.generator = generator
        self.model = self.generator.model
        self.tokenizer = self.generator.tokenizer

    def generate_until(self, requests) -> List[str]:
        """Generate greedily until a stopping sequence

        :param requests: list[Instance]
            A list of Instance objects with property `args` which returns a tuple (context, until).
            context: str
                Context string
            until: [str]
                The string sequences to generate until. These string sequences
                may each span across multiple tokens, or may be part of one token.
        :return: list[str]
            A list of strings continuationa
            continuation: str
                The generated continuation.
        """

        self.model.eval()
        results = []
        for instance in requests:
            context, until = instance.args
            context_tokens = self.tokenizer.encode(context, bos=True, eos=False)
            until_tokens = until["until"]
            prompt_tokens = context_tokens

            with torch.no_grad():
                generation_tokens, _ = self.generator.generate(
                    prompt_tokens=[prompt_tokens],
                    max_gen_len=self.model.params.max_seq_len,
                    temperature=0.6,
                    top_p=0.9,
                    logprobs=False,
                    echo=False,
                    additional_stop_tokens=until_tokens,
                )
            results.append(self.tokenizer.decode(generation_tokens[0]))

            del generation_tokens
            gc.collect()
            torch.cuda.empty_cache()

        return results

    def loglikelihood(self, requests) -> List[Tuple[float, bool]]:
        # """Compute log-likelihood of generating a continuation from a context.
        # Downstream tasks should attempt to use loglikelihood instead of other
        # LM calls whenever possible.

        # :param requests: list[Instance]
        #     A list of Instance objects, with property `args` which returns a tuple (context, continuation).
        #     `context: str`
        #         Context string. Implementations of LM must be able to handle an
        #         empty context string.
        #     `continuation: str`
        #         The continuation over which log likelihood will be calculated. If
        #         there is a word boundary, the space should be in the continuation.
        #         For example, context="hello" continuation=" world" is correct.

        # :return: list[tuple[float, bool]]
        #     A list of pairs (logprob, isgreedy)
        #     `logprob: float`
        #         The log probability of `continuation`.
        #     `isgreedy`:
        #         Whether `continuation` would be generated by greedy sampling from `context`.
        # """
        self.model.eval()
        results = []

        for request in requests:
            context, continuation = request.args

            # Encode the context and continuation
            context_tokens = self.tokenizer.encode(context, bos=True, eos=False)

            continuation_tokens = self.tokenizer.encode(
                continuation, bos=False, eos=True
            )
            input_tokens = context_tokens + continuation_tokens

            # Ensure the total length does not exceed the maximum sequence length
            if len(input_tokens) > self.model.params.max_seq_len:
                raise ValueError(
                    f"Input sequence length {len(input_tokens)} exceeds maximum sequence length {self.model.params.max_seq_len}"
                )

            # Prepare input tensor
            input_tensor = torch.tensor([input_tokens], dtype=torch.long, device="cuda")

            # Forward pass to get logits
            with torch.no_grad():
                logits = self.model(input_tensor, start_pos=0)

            # Calculate log-probabilities for the continuation part
            logits = logits[
                0, len(context_tokens) - 1 : -1
            ]  # Ignore logits for the context tokens
            continuation_tensor = torch.tensor(
                continuation_tokens, dtype=torch.long, device="cuda"
            )
            log_probs = -F.cross_entropy(logits, continuation_tensor, reduction="none")

            # Sum log-probabilities for the total log-likelihood
            log_likelihood = log_probs.sum().item()

            # Determine if the continuation is greedy (highest probability at each step)
            greedy_tokens = torch.argmax(logits, dim=-1)
            is_greedy = torch.all(greedy_tokens == continuation_tensor).item()

            results.append((log_likelihood, bool(is_greedy)))

            del (
                input_tensor,
                logits,
                log_probs,
                log_likelihood,
                continuation_tensor,
                greedy_tokens,
                is_greedy,
                continuation_tokens,
                context_tokens,
            )
            gc.collect()
            torch.cuda.empty_cache()

        return results

    def loglikelihood_rolling(self, requests) -> List[Tuple[float]]:
        # """Compute full log-likelihood of a string, with no truncation, for perplexity computation
        # - We will use the full max context length of the model.
        # - For inputs that exceed the max context length, we divide the tokenized string into chunks of up to
        # the max context length.
        # - IMPORTANT: Each document's loglikelihood/perplexity is computed *separately*, unlike other implementations
        # which may simply concatenate multiple documents together.
        # - IMPORTANT: We maximize the amount of context for each prediction. Specifically, for inputs that we break into
        # multiple chunks, the last input will still a full-sized context.
        # Example:
        #     Input tokens: [ 0 1 2 3 4 5 6 7 8 9 ]
        #     Prefix: BOS/EOS
        #     Max context length: 4
        #     Resulting input/prediction pairs:

        #         INPUT:  BOS   0   1   2
        #         PRED:     0   1   2   3

        #         INPUT:    3   4   5   6
        #         PRED:     4   5   6   7

        #         INPUT:    5   6   7   8
        #         PRED:             8   9

        # Observe that:
        #     1. Each token is predicted exactly once
        #     2. For the last pair, we provide the full context, but only score the last two tokens

        # :param requests: list[Instance]
        #     A list of Instance objects with property `args` which returns a tuple (context,).
        #     string: str
        #         String for which we are computing overall loglikelihood
        # :return: list[tuple[float]]
        #     A list of tuples (logprob,)
        #     logprob: float
        #         The log probability of `context` conditioned on the BOS/EOS token.
        #         Can also be overridden for custom cases by `prefix_token_id`.
        # """
        self.model.eval()
        results = []

        for request in requests:
            context = request.args[0]

            # Encode the context
            context_tokens = self.tokenizer.encode(context, bos=True, eos=True)
            max_context_length = self.model.params.max_seq_len

            total_logprob = 0.0

            # Process in chunks
            for i in range(0, len(context_tokens), max_context_length):
                input_tokens = context_tokens[i : i + max_context_length]
                input_tensor = torch.tensor(
                    [input_tokens], dtype=torch.long, device="cuda"
                )

                # Forward pass to get logits
                with torch.no_grad():
                    logits = self.model(input_tensor, start_pos=0)

                # Calculate log-probabilities for the input tokens
                logits = logits[0, :-1]
                target_tokens = input_tensor[0, 1:]

                log_probs = -F.cross_entropy(logits, target_tokens, reduction="none")
                total_logprob += log_probs.sum().item()

            results.append((total_logprob,))

            del (
                input_tokens,
                input_tensor,
                logits,
                log_probs,
                total_logprob,
                target_tokens,
                context_tokens,
            )
            gc.collect()
            torch.cuda.empty_cache()
        return results
